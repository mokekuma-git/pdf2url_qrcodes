{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1689ce46-3d1e-42dd-8b8b-3fcf4ee288bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "import fitz\n",
    "\n",
    "filename = \"pdf/sample20230430.pdf\"\n",
    "\n",
    "url_regrex = re.compile(r\"https?://(?:[-\\w\\/\\:\\?\\=\\~\\&\\#.]|(?:%[\\da-fA-F]{2}))+\")\n",
    "\n",
    "\n",
    "def read_url_from_pdftext(filename: str) -> Dict[str, int]:\n",
    "    text = \"\"\n",
    "    # PDFを読み込む\n",
    "    with fitz.open(filename) as doc:\n",
    "        # １ページずつテキストを抽出して連結\n",
    "        for page in range(len(doc)):\n",
    "            text += doc[page].get_text()\n",
    "\n",
    "    # URLを検索する\n",
    "    urls = url_regrex.findall(text)\n",
    "    print(f\"Url: {len(urls)}\")\n",
    "\n",
    "    # 重複チェック\n",
    "    return duplecate_check(urls)\n",
    "\n",
    "def duplecate_check(urls: List[str]) -> Dict[str, int]:\n",
    "    urls_pickuped = {}\n",
    "    counter = 0\n",
    "    for url in urls:\n",
    "        if url in urls_pickuped:\n",
    "            urls_pickuped[url] += 1\n",
    "        else:\n",
    "            urls_pickuped[url] = 1\n",
    "            counter += 1\n",
    "            # print(url)\n",
    "    print(f\"Unique url: {counter}\")\n",
    "    return urls_pickuped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67968a42-3fe2-4ac7-9417-b1f6de065a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pikepdf\n",
    "\n",
    "def read_url_from_pdflink(filename: str) -> Dict[str, int]:\n",
    "    urls = []\n",
    "    with pikepdf.Pdf.open(filename) as doc:\n",
    "        # 各ページのリンクを抽出する\n",
    "        for page in doc.pages:\n",
    "            if not page.get(\"/Annots\"):\n",
    "                continue\n",
    "            for annot in page.get(\"/Annots\"):\n",
    "                if not annot or not annot.get(\"/A\"):\n",
    "                    continue\n",
    "                uri = annot.get(\"/A\").get(\"/URI\")\n",
    "                if uri is not None:\n",
    "                    urls.append(uri)\n",
    "    print(f\"Url: {len(urls)}\")\n",
    "\n",
    "    # 重複チェック\n",
    "    return duplecate_check(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a8763b-d3b9-40cc-ade1-1b95efecf810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url: 84\n",
      "Unique url: 84\n",
      "Url: 85\n",
      "Unique url: 85\n"
     ]
    }
   ],
   "source": [
    "urls_pickuped = read_url_from_pdftext(filename)\n",
    "urls_linked = read_url_from_pdflink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb76364-5b49-4775-a14e-429fee974535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add': [(pikepdf.String(\"https://ja.wikipedia.org/wiki/%E5%8A%89%E8%A3%95\"), 1), (pikepdf.String(\"http://www.kbs-kyoto.co.jp/tv/kaigai/archives/hokugi/%23062378\"), 1)], 'remove': [('http://www.kbs-kyoto.co.jp/tv/kaigai/archives/hokugi/#062378', 1)]}\n"
     ]
    }
   ],
   "source": [
    "from dictdiffer import diff\n",
    "\n",
    "result = {\n",
    "    x: _list for (x, y, _list) in (diff(urls_pickuped, urls_linked))\n",
    "}\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869ee27d-4e52-41a4-86a9-758e022f0651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ja.wikipedia.org/wiki/劉裕 not found\n",
      "http://www.kbs-kyoto.co.jp/tv/kaigai/archives/hokugi/#062378 found\n"
     ]
    }
   ],
   "source": [
    "# マージ\n",
    "\n",
    "urls = [x for x in urls_pickuped.keys()]\n",
    "#urls.append(str(result[\"add\"][0][0]))\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "for (url, count) in result[\"add\"]:\n",
    "    decoded = urllib.parse.unquote(str(url))\n",
    "    if decoded in urls:\n",
    "        print(f\"{decoded} found\")\n",
    "    else:\n",
    "        print(f\"{decoded} not found\")\n",
    "        urls.append(str(url))\n",
    "\n",
    "with open(\"urls.txt\", mode=\"w\") as _fp:\n",
    "    for url in urls:\n",
    "        _fp.write(url)\n",
    "        _fp.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95539ef9-0b80-445d-9354-7e945e2e8a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import qrcode\n",
    "\n",
    "# import qrcode.image.svg\n",
    "\n",
    "# method = \"basic\"\n",
    "\n",
    "# if not method and method == 'basic':\n",
    "#     # Simple factory, just a set of rects.\n",
    "#     factory = qrcode.image.svg.SvgImage\n",
    "# elif method == 'fragment':\n",
    "#     # Fragment factory (also just a set of rects)\n",
    "#     factory = qrcode.image.svg.SvgFragmentImage\n",
    "# else:\n",
    "#     # Combined path factory, fixes white space that may occur when zooming\n",
    "#     factory = qrcode.image.svg.SvgPathImage\n",
    "\n",
    "factory = qrcode.image.pure.PyPNGImage\n",
    "\n",
    "qr = qrcode.QRCode(\n",
    "    version=1,\n",
    "    error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "    box_size=2,\n",
    "    border=4,\n",
    "    image_factory=factory\n",
    ")\n",
    "\n",
    "def make_qrcodes(urls: List[str]) -> None:\n",
    "    for url in urls:\n",
    "        qr.clear()\n",
    "        qr.add_data(url)\n",
    "        qr.make(fit=True)\n",
    "        image = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
    "        if image:\n",
    "            image.save(\"qrcode/\" + re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", url) + \".png\")\n",
    "        else:\n",
    "            print(f\"image empty {image} {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e6b0ac-afbc-44f3-9e66-6b202058b5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def clear_dir(dirname: str) -> None:\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "def zip_dir(zip_file: str, dirname: str, more_files: List[str]) -> None:\n",
    "    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # ディレクトリ内のファイルをzipに追加する\n",
    "        for foldername, subfolders, filenames in os.walk(dirname):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(foldername, filename)\n",
    "                zipf.write(filepath, os.path.relpath(filepath, dirname))\n",
    "        # 追加のファイルをzipに追加する\n",
    "        for file in more_files:\n",
    "            zipf.write(file, os.path.basename(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef276351-9509-4cc6-a193-6ec3269e7430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qrcode_dir = \"qrcode\"\n",
    "more_files = [\"urls.txt\"]\n",
    "zip_file = \"qrcodes.zip\"\n",
    "\n",
    "clear_dir(qrcode_dir)\n",
    "make_qrcodes(urls)\n",
    "zip_dir(zip_file, qrcode_dir, more_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a424a-909a-4a61-a74c-8d0dd8b3dd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
